- echo: ğŸ§ª Structured Evaluation Echo
  id: evaluation-structured
  mode: Evaluative

  purpose: >-
    To issue a structured judgment on an object (idea, result, process, code, or design),
    using clear and validated criteria. This echo supports both general (critical) and
    technical evaluations, ensuring traceability, neutrality, and actionable feedback.

  trigger: >-
    Triggered when evaluating any object (submission, result, proposal, technical artifact)
    to determine its quality, effectiveness, or alignment with expectations.

  config:
    retry_on_fail: true
    max_retries_per_step: 3

  steps:
    - name: Identification of the object to evaluate
      goal: >-
        Clearly identify the object to be evaluated, its purpose, and the reason for evaluation.
      validation:
        - Is the object explicitly named?
        - Is its purpose or context explained?
        - Is the reason for evaluating it stated?
      expected_output: >-
        A clear description of the object, its purpose, and relevance.

    - name: Selection of evaluation mode
      goal: >-
        Decide whether the evaluation will be general/critical or technical,
        and adapt the following analysis accordingly.
        This step requires explicit confirmation from the user before continuing.
      validation:
        - Was a mode suggested by the system?
        - Did the user confirm or modify it?
        - Were the following criteria adapted accordingly?
      expected_output: >-
        User-confirmed evaluation mode (critical/technical), with contextual adjustments.
      wait_for_user_input: true

    - name: Definition of evaluation criteria
      goal: >-
        Establish the specific criteria to be used for judgment.
        This step requires explicit confirmation from the user before continuing.
      validation:
        - Were criteria suggested by the system?
        - Did the user confirm or modify them?
      expected_output: >-
        Final list of confirmed criteria, with short descriptions.
      wait_for_user_input: true

    - name: Evaluation per criterion
      goal: >-
        Analyze the object according to each criterion, highlighting strengths and weaknesses.
      validation:
        - Was each criterion addressed?
        - Was each judgment justified with reasoning or evidence?
      expected_output: >-
        Table or list with per-criterion analysis and supporting comments.

    - name: Global judgment and recommendation
      goal: >-
        Deliver an overall conclusion and an actionable recommendation (e.g., keep, revise, improve),
        justified by the findings.
      validation:
        - Was a global conclusion given?
        - Was a recommendation issued based on the evaluation?
        - Is the recommendation aligned with the criteria applied?
      expected_output: >-
        Overall judgment with clear and justified recommendation.

  output_format: >-
    Structured evaluation with:
      â€¢ evaluated object description
      â€¢ confirmed evaluation mode
      â€¢ confirmed criteria
      â€¢ per-criterion analysis
      â€¢ final recommendation

  notes: |-
    This echo unifies the former "Critical Evaluation" and "Technical Review" echoes.
    It supports two submodes:
      â€¢ Critical â€“ to assess proposals, ideas, outcomes, or general results
      â€¢ Technical â€“ to assess code, design, architecture, or systems

    Example uses:
      â€¢ Course proposal evaluation: goal clarity, pedagogical utility, feasibility.
      â€¢ React component review: code quality, performance, architecture alignment.

    This echo pauses after steps 2 and 3 for user confirmation before continuing.

    Compatible with:
      â€¢ ğŸ§ Metaevaluation â€“ to audit past judgments
      â€¢ âš™ï¸ Optimization â€“ to refine evaluated outputs
      â€¢ ğŸ› ï¸ Diagnostic â€“ to detect deeper structural flaws
